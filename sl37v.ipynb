{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8917af52-516a-4f97-ae38-9cc085a90805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download tokenizer models (only once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "block = \"CSI DYPIEMR is the student chapter of the Computer Society of India in Dr. D. Y. Patil Pratishthan's Dr. D. Y. Patil Institute of Engineering Management and Research. Computer Society of India is a body of computer professionals in India. It was started on 6 March 1965 by a few computer professionals and has now grown to be the national body representing computer professionals. It has 72 chapters across India, and 511 student branches.\"\n",
    "\n",
    "# Word-wise tokenization\n",
    "print(\"This is word-wise tokenization:\\n\")\n",
    "print(word_tokenize(block))\n",
    "print(\"\\n------------------------------------------------------\")\n",
    "\n",
    "# Sentence-wise tokenization\n",
    "print(\"\\nThis is sentence-wise tokenization:\\n\")\n",
    "print(sent_tokenize(block))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f668632-6421-4426-b6c4-7c477be7fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d27d3ec-8037-4649-a80a-f8fdec83211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')  # Required for tokenization\n",
    "nltk.download('stopwords')  # This is where your issue is\n",
    "\n",
    "block = \"CSI DYPIEMR is the student chapter of the Computer Society of India in Dr. D. Y. Patil Pratishthan's Dr. D. Y. Patil Institute of Engineering Management and Research. Computer Society of India is a body of computer professionals in India. It was started on 6 March 1965 by a few computer professionals and has now grown to be the national body representing computer professionals. It has 72 chapters across India, and 511 student branches.\"\n",
    "stop_words = stopwords.words('english')\n",
    "print(\"Stopwords:\", stop_words)\n",
    "\n",
    "token = word_tokenize(block)\n",
    "cleaned_token = []\n",
    "\n",
    "for word in token:\n",
    "    if word.lower() not in stop_words:\n",
    "        cleaned_token.append(word)\n",
    "\n",
    "print(\"This is the unclean version:\\n\", token, '\\n')\n",
    "print(\"-------------------------------------------------------\\n\")\n",
    "print(\"This is the cleaned version:\\n\", cleaned_token, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e173670-8832-48eb-a2a8-15a4fc04c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer=nltk.PorterStemmer()\n",
    "words=['rain','rained','raining','rains']\n",
    "stemmed=[stemmer.stem(word) for word in words]\n",
    "print(stemmed)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644c046-cbf7-449b-bbb2-8a2f16ac1182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer=nltk.WordNetLemmatizer()\n",
    "lemmatized=[lemmatizer.lemmatize(word) for word in cleaned_token]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b8257-c1a0-4948-a6e8-9223eefe5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tagged=nltk.pos_tag(cleaned_token)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf682281-5ed2-43ae-a9df-552e8df18c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d2a0e-024e-4065-bfc0-ad0dc0746bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_1 = \"Our aim is to develop a good work culture among students, a culture where students from various technical backgrounds\"\n",
    "block_2 = \"Keeping in mind the interest of the IT professionals and computer enthusiasts, CSI works towards making the profession\"\n",
    "\n",
    "#split so each word have their own string\n",
    "first_block = block_1.split(\" \")\n",
    "second_block = block_2.split(\" \")\n",
    "\n",
    "#join them to remove common duplicate words\n",
    "total = set(first_block).union(set(second_block))\n",
    "print(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3729c8-3280-46b8-b720-ef9f8de6be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDictA = dict.fromkeys(total, 0)\n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "\n",
    "for word in first_block:\n",
    "    wordDictA[word] += 1\n",
    "\n",
    "for word in second_block:\n",
    "    wordDictB[word] += 1\n",
    "\n",
    "pd.DataFrame([wordDictA, wordDictB])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece66e86-ca06-4af9-8a72-6d126db24c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_sentence = [w for w in wordDictA if w not in stop_words]\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cd9f14-b962-41a1-8f1c-5a6708f2208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(corpusCount)\n",
    "    return tfDict\n",
    "\n",
    "# Running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, first_block)\n",
    "tfSecond = computeTF(wordDictB, second_block)\n",
    "tf = pd.DataFrame([tfFirst, tfSecond])\n",
    "print(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbbeb20-5a86-485e-8d95-93f480ea04b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / (float(val) + 1))\n",
    "    \n",
    "    return idfDict\n",
    "\n",
    "idfs = computeIDF([wordDictA, wordDictB])\n",
    "idf1 = pd.DataFrame([wordDictA, wordDictB])\n",
    "print(idf1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace3bd68-07f7-4a2e-9523-b9678fff2b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "# Running our two sentences through the IDF:\n",
    "idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "\n",
    "# Putting it in a DataFrame\n",
    "idf = pd.DataFrame([idfFirst, idfSecond])\n",
    "print(idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a4563-36b8-4aff-b680-6d7617e4449f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5491e8-32e9-499f-a00e-7bf798ad0c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfad025-024f-46c7-8a5f-723b3e34e01d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
