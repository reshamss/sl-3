{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa437b1-16b0-471e-8d13-3cad921147ec",
   "metadata": {},
   "source": [
    "### Text Analytics\n",
    "\n",
    "- 1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS\n",
    "Tagging, stop words removal, Stemming and Lemmatization.\n",
    "- 2. Create representation of documents by calculating Term Frequency and Inverse Document Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32fee96d-a4cd-4200-8022-62927c9ed42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27eef9d-7c96-475a-9da5-af347ff22a58",
   "metadata": {},
   "source": [
    "#### 1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4ef1386-809f-49f2-b83b-b9916a7dbfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word wise tokenization:  ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '(', 'AI', ')', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language', '.', 'It', 'involves', 'the', 'analysis', ',', 'understanding', ',', 'and', 'generation', 'of', 'human', 'language', ',', 'enabling', 'machines', 'to', 'process', 'and', 'comprehend', 'text', 'in', 'a', 'meaningful', 'way', '.', 'NLP', 'techniques', 'are', 'widely', 'used', 'in', 'various', 'applications', 'such', 'as', 'sentiment', 'analysis', ',', 'machine', 'translation', ',', 'chatbots', ',', 'and', 'information', 'retrieval', '.', 'Preprocessing', 'is', 'an', 'essential', 'step', 'in', 'NLP', ',', 'which', 'involves', 'tokenization', ',', 'part-of-speech', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n",
      "\n",
      " ------------------------------ \n",
      "\n",
      "Sentence wise tokenization:  ['Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans using natural language.', 'It involves the analysis, understanding, and generation of human language, enabling machines to process and comprehend text in a meaningful way.', 'NLP techniques are widely used in various applications such as sentiment analysis, machine translation, chatbots, and information retrieval.', 'Preprocessing is an essential step in NLP, which involves tokenization, part-of-speech tagging, stop words removal, stemming, and lemmatization.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "block = \"Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans using natural language. It involves the analysis, understanding, and generation of human language, enabling machines to process and comprehend text in a meaningful way. NLP techniques are widely used in various applications such as sentiment analysis, machine translation, chatbots, and information retrieval. Preprocessing is an essential step in NLP, which involves tokenization, part-of-speech tagging, stop words removal, stemming, and lemmatization.\"\n",
    "\n",
    "print(\"Word wise tokenization: \", nltk.word_tokenize(block))\n",
    "print(\"\\n\",\"-\"*30,\"\\n\")\n",
    "print(\"Sentence wise tokenization: \", nltk.sent_tokenize(block))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3033366-2f65-48e4-af1b-ba601a00387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words:  ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "\n",
      " ------------------------------ \n",
      "\n",
      "Uncleaned token:  ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '(', 'AI', ')', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language', '.', 'It', 'involves', 'the', 'analysis', ',', 'understanding', ',', 'and', 'generation', 'of', 'human', 'language', ',', 'enabling', 'machines', 'to', 'process', 'and', 'comprehend', 'text', 'in', 'a', 'meaningful', 'way', '.', 'NLP', 'techniques', 'are', 'widely', 'used', 'in', 'various', 'applications', 'such', 'as', 'sentiment', 'analysis', ',', 'machine', 'translation', ',', 'chatbots', ',', 'and', 'information', 'retrieval', '.', 'Preprocessing', 'is', 'an', 'essential', 'step', 'in', 'NLP', ',', 'which', 'involves', 'tokenization', ',', 'part-of-speech', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n",
      "\n",
      " ------------------------------ \n",
      "\n",
      "Cleaned token:  ['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'artificial', 'intelligence', '(', 'AI', ')', 'focuses', 'interaction', 'computers', 'humans', 'using', 'natural', 'language', '.', 'It', 'involves', 'analysis', ',', 'understanding', ',', 'generation', 'human', 'language', ',', 'enabling', 'machines', 'process', 'comprehend', 'text', 'meaningful', 'way', '.', 'NLP', 'techniques', 'widely', 'used', 'various', 'applications', 'sentiment', 'analysis', ',', 'machine', 'translation', ',', 'chatbots', ',', 'information', 'retrieval', '.', 'Preprocessing', 'essential', 'step', 'NLP', ',', 'involves', 'tokenization', ',', 'part-of-speech', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stop words removal\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "print(\"Stop Words: \", stop_words)\n",
    "\n",
    "token = nltk.word_tokenize(block)\n",
    "cleaned_token = [word for word in token if word not in stop_words]\n",
    "\n",
    "print(\"\\n\",\"-\"*30,\"\\n\")\n",
    "print(\"Uncleaned token: \", token)\n",
    "print(\"\\n\",\"-\"*30,\"\\n\")\n",
    "print(\"Cleaned token: \", cleaned_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bcc1480a-7549-40ce-8b6b-8083493e69e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rain', 'rain', 'rain', 'rain']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = nltk.PorterStemmer()\n",
    "words = [\"rain\", \"rained\", \"raining\", \"rains\"]\n",
    "stemmed = [stemmer.stem(word) for word in words]\n",
    "stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f7238eb-fa41-46be-b11b-7de685384881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'artificial', 'intelligence', '(', 'AI', ')', 'focus', 'interaction', 'computer', 'human', 'using', 'natural', 'language', '.', 'It', 'involves', 'analysis', ',', 'understanding', ',', 'generation', 'human', 'language', ',', 'enabling', 'machine', 'process', 'comprehend', 'text', 'meaningful', 'way', '.', 'NLP', 'technique', 'widely', 'used', 'various', 'application', 'sentiment', 'analysis', ',', 'machine', 'translation', ',', 'chatbots', ',', 'information', 'retrieval', '.', 'Preprocessing', 'essential', 'step', 'NLP', ',', 'involves', 'tokenization', ',', 'part-of-speech', 'tagging', ',', 'stop', 'word', 'removal', ',', 'stemming', ',', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in cleaned_token]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8d6d932-4d2a-4d48-a39f-a3b6ddb2fe6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('subfield', 'VBD'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('(', '('), ('AI', 'NNP'), (')', ')'), ('focuses', 'VBZ'), ('interaction', 'JJ'), ('computers', 'NNS'), ('humans', 'NNS'), ('using', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('.', '.'), ('It', 'PRP'), ('involves', 'VBZ'), ('analysis', 'NN'), (',', ','), ('understanding', 'NN'), (',', ','), ('generation', 'NN'), ('human', 'JJ'), ('language', 'NN'), (',', ','), ('enabling', 'VBG'), ('machines', 'NNS'), ('process', 'NN'), ('comprehend', 'VBP'), ('text', 'NN'), ('meaningful', 'JJ'), ('way', 'NN'), ('.', '.'), ('NLP', 'NNP'), ('techniques', 'VBZ'), ('widely', 'RB'), ('used', 'VBN'), ('various', 'JJ'), ('applications', 'NNS'), ('sentiment', 'NN'), ('analysis', 'NN'), (',', ','), ('machine', 'NN'), ('translation', 'NN'), (',', ','), ('chatbots', 'NNS'), (',', ','), ('information', 'NN'), ('retrieval', 'NN'), ('.', '.'), ('Preprocessing', 'VBG'), ('essential', 'JJ'), ('step', 'NN'), ('NLP', 'NNP'), (',', ','), ('involves', 'VBZ'), ('tokenization', 'NN'), (',', ','), ('part-of-speech', 'JJ'), ('tagging', 'NN'), (',', ','), ('stop', 'VB'), ('words', 'NNS'), ('removal', 'JJ'), (',', ','), ('stemming', 'VBG'), (',', ','), ('lemmatization', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging\n",
    "from nltk import pos_tag\n",
    "tagged = nltk.pos_tag(cleaned_token)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104060b0-b093-4459-8d92-eb45aebd6dde",
   "metadata": {},
   "source": [
    "#### 2. Create representation of documents by calculating Term Frequency and Inverse Document Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8dec019-b4c7-4dc0-a2a0-f920d5b323ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9bf27c1-f946-4bc7-b82b-5c7fe54c5ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data', 'machines', 'With', 'customer', 'for', 'powers', 'AI-driven', 'as', 'industries', 'wide', 'aiding', 'especially', 'data.', 'can', 'provide', 'Moreover,', 'modern', 'a', 'Processing', 'be', 'sentiment', 'finds', 'enables', 'NLP', 'and', 'text', 'monitoring.', 'automate', 'such', 'interactions.', 'content', 'making', 'tools,', 'cases.', 'applications.', 'virtual', 'chatbots', 'recommendations,', 'language,', 'analyzing', 'insights', 'diagnostics', 'structured', 'data,', 'actionable', 'entity', 'science,', 'into', 'keyword', 'trends,', 'transformed', 'applications', 'In', 'to', 'topic', 'allowing', 'Natural', 'use', 'text—from', 'extract', 'recognition,', 'that', 'NLP,', 'plays', 'patterns,', 'better', 'assistants,', 'tasks', 'is', 'analysis,', 'notes,', 'extracting', 'media', 'uncover', 'component', 'healthcare,', 'volumes', 'vast', 'helps', 'critical', 'meaningful', 'understand,', 'care.', 'patient', 'used', 'compliance', 'interpret,', 'named', 'fraud', 'of', 'clinical', 'information', 'decision-making.', 'detection,', 'essential', 'real-time', 'widely', 'perform', 'social', 'also', '(NLP)', 'posts', 'voice', 'scientists', 'easier', 'extraction.', 'in', 'translation', 'It', 'generate', 'role', 'document', 'an', 'language', 'across', 'it', 'from', 'support', 'modeling,', 'finance', 'reviews', 'personalized', 'service,', 'range', 'unstructured', 'assistants', 'human', 'Language'}\n"
     ]
    }
   ],
   "source": [
    "block_1 = \"Natural Language Processing (NLP) plays a critical role in data science, especially in analyzing and extracting insights from unstructured text data. It enables machines to understand, interpret, and generate human language, allowing data scientists to perform tasks such as sentiment analysis, topic modeling, named entity recognition, and keyword extraction. With NLP, vast volumes of text—from customer reviews to social media posts can be transformed into structured data, making it easier to uncover patterns, trends, and actionable insights that support better decision-making.\"\n",
    "block_2 = \"NLP finds applications across a wide range of industries and use cases. In customer service, chatbots and virtual assistants use NLP to provide real-time support and automate interactions. In healthcare, NLP helps extract meaningful information from clinical notes, aiding in diagnostics and patient care. It is also widely used in finance for fraud detection, document analysis, and compliance monitoring. Moreover, NLP powers language translation tools, voice assistants, and personalized content recommendations, making it an essential component of modern AI-driven applications.\"\n",
    "\n",
    "first_block = block_1.split(\" \")\n",
    "second_block = block_2.split(\" \")\n",
    "\n",
    "total = set(first_block).union(set(second_block))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe4b2d51-469c-4d8d-bffd-4e27e6fc929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDictA = dict.fromkeys(total, 0)\n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "\n",
    "for word in first_block: wordDictA[word] += 1\n",
    "for word in second_block: wordDictB[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6c21680-5152-45a6-9da5-795a0dbfe059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>machines</th>\n",
       "      <th>With</th>\n",
       "      <th>customer</th>\n",
       "      <th>for</th>\n",
       "      <th>powers</th>\n",
       "      <th>AI-driven</th>\n",
       "      <th>as</th>\n",
       "      <th>industries</th>\n",
       "      <th>wide</th>\n",
       "      <th>...</th>\n",
       "      <th>modeling,</th>\n",
       "      <th>finance</th>\n",
       "      <th>reviews</th>\n",
       "      <th>personalized</th>\n",
       "      <th>service,</th>\n",
       "      <th>range</th>\n",
       "      <th>unstructured</th>\n",
       "      <th>assistants</th>\n",
       "      <th>human</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   data  machines  With  customer  for  powers  AI-driven  as  industries  \\\n",
       "0     2         1     1         1    0       0          0   1           0   \n",
       "1     0         0     0         1    1       1          1   0           1   \n",
       "\n",
       "   wide  ...  modeling,  finance  reviews  personalized  service,  range  \\\n",
       "0     0  ...          1        0        1             0         0      0   \n",
       "1     1  ...          0        1        0             1         1      1   \n",
       "\n",
       "   unstructured  assistants  human  Language  \n",
       "0             1           0      1         1  \n",
       "1             0           1      0         0  \n",
       "\n",
       "[2 rows x 128 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be587f30-04ec-4210-b932-b3066bfd39fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'machines', 'With', 'customer', 'powers', 'AI-driven', 'industries', 'wide', 'aiding', 'especially', 'data.', 'provide', 'Moreover,', 'modern', 'Processing', 'sentiment', 'finds', 'enables', 'NLP', 'text', 'monitoring.', 'automate', 'interactions.', 'content', 'making', 'tools,', 'cases.', 'applications.', 'virtual', 'chatbots', 'recommendations,', 'language,', 'analyzing', 'insights', 'diagnostics', 'structured', 'data,', 'actionable', 'entity', 'science,', 'keyword', 'trends,', 'transformed', 'applications', 'In', 'topic', 'allowing', 'Natural', 'use', 'text—from', 'extract', 'recognition,', 'NLP,', 'plays', 'patterns,', 'better', 'assistants,', 'tasks', 'analysis,', 'notes,', 'extracting', 'media', 'uncover', 'component', 'healthcare,', 'volumes', 'vast', 'helps', 'critical', 'meaningful', 'understand,', 'care.', 'patient', 'used', 'compliance', 'interpret,', 'named', 'fraud', 'clinical', 'information', 'decision-making.', 'detection,', 'essential', 'real-time', 'widely', 'perform', 'social', 'also', '(NLP)', 'posts', 'voice', 'scientists', 'easier', 'extraction.', 'translation', 'It', 'generate', 'role', 'document', 'language', 'across', 'support', 'modeling,', 'finance', 'reviews', 'personalized', 'service,', 'range', 'unstructured', 'assistants', 'human', 'Language']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "filtered_sentence = [w for w in wordDictA if w not in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb889877-8c8b-4bfb-9dbe-594c441fc64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       data  machines      With  customer     for  powers  AI-driven  \\\n",
      "0  0.024691  0.012346  0.012346  0.012346  0.0000  0.0000     0.0000   \n",
      "1  0.000000  0.000000  0.000000  0.012500  0.0125  0.0125     0.0125   \n",
      "\n",
      "         as  industries    wide  ...  modeling,  finance   reviews  \\\n",
      "0  0.012346      0.0000  0.0000  ...   0.012346   0.0000  0.012346   \n",
      "1  0.000000      0.0125  0.0125  ...   0.000000   0.0125  0.000000   \n",
      "\n",
      "   personalized  service,   range  unstructured  assistants     human  \\\n",
      "0        0.0000    0.0000  0.0000      0.012346      0.0000  0.012346   \n",
      "1        0.0125    0.0125  0.0125      0.000000      0.0125  0.000000   \n",
      "\n",
      "   Language  \n",
      "0  0.012346  \n",
      "1  0.000000  \n",
      "\n",
      "[2 rows x 128 columns]\n"
     ]
    }
   ],
   "source": [
    "def computeTF(wordDict, doc) :\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items(): tfDict[word] = count/corpusCount\n",
    "    return tfDict\n",
    "\n",
    "tfFirst = computeTF(wordDictA, first_block)\n",
    "tfSecond = computeTF(wordDictB, second_block)\n",
    "\n",
    "tf = pd.DataFrame([tfFirst, tfSecond])\n",
    "\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "112cfe35-bf81-4922-9cbe-7e3f55088960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   data  machines  With  customer  for  powers  AI-driven  as  industries  \\\n",
      "0     2         1     1         1    0       0          0   1           0   \n",
      "1     0         0     0         1    1       1          1   0           1   \n",
      "\n",
      "   wide  ...  modeling,  finance  reviews  personalized  service,  range  \\\n",
      "0     0  ...          1        0        1             0         0      0   \n",
      "1     1  ...          0        1        0             1         1      1   \n",
      "\n",
      "   unstructured  assistants  human  Language  \n",
      "0             1           0      1         1  \n",
      "1             0           1      0         0  \n",
      "\n",
      "[2 rows x 128 columns]\n"
     ]
    }
   ],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items(): idfDict[word] = math.log10(N / (float(val) + 1))\n",
    "    return idfDict\n",
    "\n",
    "idfs = computeIDF([wordDictA, wordDictB])\n",
    "idfs1 = pd.DataFrame([wordDictA, wordDictB])\n",
    "print(idfs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b6855-135a-4e7a-b26f-ae1b040c320c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
